{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "51379365-1a60-471a-bf56-07979d66e19d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA_HOME = /packages/apps/spack/21/opt/spack/linux-rocky8-zen3/gcc-12.1.0/cuda-12.0.1-x3bnvayrybncl3rqu6zk4zzu4oztblqi\n",
      "which nvcc -> /packages/apps/spack/21/opt/spack/linux-rocky8-zen3/gcc-12.1.0/cuda-12.0.1-x3bnvayrybncl3rqu6zk4zzu4oztblqi/bin/nvcc\n",
      "/packages/apps/spack/21/opt/spack/linux-rocky8-zen3/gcc-12.1.0/cuda-12.0.1-x3bnvayrybncl3rqu6zk4zzu4oztblqi/bin/nvcc\n",
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2023 NVIDIA Corporation\n",
      "Built on Fri_Jan__6_16:45:21_PST_2023\n",
      "Cuda compilation tools, release 12.0, V12.0.140\n",
      "Build cuda_12.0.r12.0/compiler.32267302_0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os, subprocess, shutil\n",
    "\n",
    "# replace this with the nvcc path you posted (we compute CUDA_HOME automatically)\n",
    "nvcc_path = \"/packages/apps/spack/21/opt/spack/linux-rocky8-zen3/gcc-12.1.0/cuda-12.0.1-x3bnvayrybncl3rqu6zk4zzu4oztblqi/bin/nvcc\"\n",
    "\n",
    "if not os.path.exists(nvcc_path):\n",
    "    raise FileNotFoundError(f\"nvcc not found at {nvcc_path} â€” update nvcc_path if different\")\n",
    "\n",
    "cuda_home = os.path.dirname(os.path.dirname(nvcc_path))\n",
    "os.environ[\"CUDA_HOME\"] = cuda_home\n",
    "os.environ[\"PATH\"] = os.path.join(cuda_home, \"bin\") + os.pathsep + os.environ.get(\"PATH\", \"\")\n",
    "# Append existing LD_LIBRARY_PATH if present\n",
    "os.environ[\"LD_LIBRARY_PATH\"] = os.path.join(cuda_home, \"lib64\") + os.pathsep + os.environ.get(\"LD_LIBRARY_PATH\", \"\")\n",
    "\n",
    "print(\"CUDA_HOME =\", cuda_home)\n",
    "print(\"which nvcc ->\", shutil.which(\"nvcc\"))\n",
    "\n",
    "# quick verify (runs in a shell inheriting these env vars)\n",
    "proc = subprocess.run(\"/bin/bash -lc 'which nvcc && nvcc --version'\", shell=True, capture_output=True, text=True, env=os.environ)\n",
    "print(proc.stdout)\n",
    "if proc.returncode != 0:\n",
    "    print(\"nvcc failed to run; stderr:\\n\", proc.stderr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2a90cea0-b07e-4c64-b16b-23fcdbc0a679",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Neural Network Forward Pass on GPU ---\n",
      "M=128, D_in=64, D_hid=32, C=10\n",
      "\n",
      "[L1] 1. Executing GEMM + Bias (X * W1 + B1)...\n",
      "[L1] 2. Executing ReLU Activation...\n",
      "[L2] 3. Executing GEMM + Bias (A1 * W2 + B2)...\n",
      "[L2] 4. Executing Softmax Activation...\n",
      "[Loss] 5. Executing Cross-Entropy (Element-wise Loss)...\n",
      "[Loss] 6. Executing Reduction (Summing Loss)...\n",
      "\n",
      "--- Forward Pass Complete ---\n",
      "Total Loss Sum: 294.731\n",
      "Average Cross-Entropy Loss (Final Output): 2.30259\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "export CUDA_HOME=/packages/apps/spack/21/opt/spack/linux-rocky8-zen3/gcc-12.1.0/cuda-12.0.1-x3bnvayrybncl3rqu6zk4zzu4oztblqi\n",
    "export PATH=$CUDA_HOME/bin:$PATH\n",
    "export LD_LIBRARY_PATH=$CUDA_HOME/lib64:$LD_LIBRARY_PATH\n",
    "cat > forward_pass.cu <<'EOF'\n",
    "#include <iostream>\n",
    "#include <vector>\n",
    "#include <cmath>\n",
    "#include <numeric>\n",
    "\n",
    "// Macro for error checking\n",
    "#define DIV_UP(a, b) (((a) + (b) - 1) / (b))\n",
    "\n",
    "#define CUDA_CHECK(call)                                                          \\\n",
    "{                                                                                 \\\n",
    "    cudaError_t err = call;                                                       \\\n",
    "    if (err != cudaSuccess) {                                                     \\\n",
    "        fprintf(stderr, \"CUDA error at %s:%d code=%d(%s) \\\"%s\\\"\\n\",               \\\n",
    "                __FILE__, __LINE__, err, cudaGetErrorString(err), #call);         \\\n",
    "        exit(EXIT_FAILURE);                                                       \\\n",
    "    }                                                                             \\\n",
    "}\n",
    "\n",
    "// Global constants for the network size\n",
    "const int M = 128;   // Batch Size (Rows)\n",
    "const int DIN = 64;  // Input Dimension\n",
    "const int DHID = 32; // Hidden Dimension\n",
    "const int C = 10;    // Number of Classes (Output Dimension)\n",
    "\n",
    "const int THREADS_PER_BLOCK = 256;\n",
    "\n",
    "// --- 1. Initialization and Data Setup Kernels ---\n",
    "\n",
    "__global__ void initWeightsKernel(float *d_data, int size) {\n",
    "    int i = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (i < size) {\n",
    "        // Use a simple pseudo-random number generation based on thread index\n",
    "        // In a real application, a more robust RNG like cuRAND would be used\n",
    "        unsigned int seed = i;\n",
    "        float rand_val = (float)seed / (float)0xFFFFFFFF; // Scale to 0 to 1\n",
    "        d_data[i] = (rand_val - 0.5f); // Scale to -0.5 to 0.5\n",
    "    }\n",
    "}\n",
    "\n",
    "// --- 2. Dense Layer (GEMM + Bias Addition) ---\n",
    "\n",
    "__global__ void simpleGemmBiasKernel(const float *X, const float *W, const float *B, float *Z, \n",
    "                                     int M, int K, int N) {\n",
    "    // Thread index for output matrix Z (row, col)\n",
    "    int row = blockIdx.y * blockDim.y + threadIdx.y;\n",
    "    int col = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "\n",
    "    if (row < M && col < N) {\n",
    "        float sum = 0.0f;\n",
    "        // Perform the dot product of X[row, :] and W[:, col]\n",
    "        for (int k = 0; k < K; ++k) {\n",
    "            sum += X[row * K + k] * W[k * N + col];\n",
    "        }\n",
    "        \n",
    "        // Add the bias (B is a 1D vector added to every row)\n",
    "        sum += B[col];\n",
    "        \n",
    "        Z[row * N + col] = sum;\n",
    "    }\n",
    "}\n",
    "\n",
    "// --- 3. Activation Functions ---\n",
    "\n",
    "__global__ void reluKernel(float *A, int size) {\n",
    "    int i = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (i < size) {\n",
    "        if (A[i] < 0.0f) {\n",
    "            A[i] = 0.0f;\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "__global__ void softmaxKernel(float *Z, int M, int C) {\n",
    "    // One thread per row for calculation\n",
    "    int row = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "\n",
    "    if (row < M) {\n",
    "        float max_val = -1e20f;\n",
    "        float sum_exp = 0.0f;\n",
    "        int row_start = row * C;\n",
    "        \n",
    "        // Find max for numerical stability (simple loop in this simplified kernel)\n",
    "        for (int j = 0; j < C; ++j) {\n",
    "            max_val = fmaxf(max_val, Z[row_start + j]);\n",
    "        }\n",
    "        \n",
    "        for (int j = 0; j < C; ++j) {\n",
    "            Z[row_start + j] = expf(Z[row_start + j] - max_val);\n",
    "            sum_exp += Z[row_start + j];\n",
    "        }\n",
    "\n",
    "        for (int j = 0; j < C; ++j) {\n",
    "            Z[row_start + j] /= sum_exp;\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "// --- 4. Loss Computation ---\n",
    "\n",
    "__global__ void crossEntropyKernel(const float *Y_hat, const int *Y_true, float *d_loss_elements, \n",
    "                                   int M, int C) {\n",
    "    // One thread per input sample (row)\n",
    "    int i = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "\n",
    "    if (i < M) {\n",
    "        int true_class = Y_true[i];\n",
    "        float predicted_prob = Y_hat[i * C + true_class];\n",
    "        \n",
    "        float loss = -logf(fmaxf(predicted_prob, 1e-9f)); \n",
    "        \n",
    "        d_loss_elements[i] = loss;\n",
    "    }\n",
    "}\n",
    "\n",
    "/**\n",
    " * @brief Performs a generic parallel sum reduction (similar to earlier examples).\n",
    " * * @param d_in Input array\n",
    " * @param d_out Output array (size 1)\n",
    " * @param size Size of the input array\n",
    " */\n",
    "__global__ void reduceSumKernel(float *d_in, float *d_out, int size) {\n",
    "    // Using a simple block-wise reduction for demonstration\n",
    "    __shared__ float sdata[THREADS_PER_BLOCK];\n",
    "    \n",
    "    int tid = threadIdx.x;\n",
    "    int i = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    \n",
    "    // Load into shared memory\n",
    "    sdata[tid] = (i < size) ? d_in[i] : 0.0f;\n",
    "    __syncthreads();\n",
    "\n",
    "    // Reduction in shared memory\n",
    "    for (int s = blockDim.x / 2; s > 0; s >>= 1) {\n",
    "        if (tid < s) {\n",
    "            sdata[tid] += sdata[tid + s];\n",
    "        }\n",
    "        __syncthreads();\n",
    "    }\n",
    "    \n",
    "    // Write result to global memory (atomic not strictly needed here if grid is small)\n",
    "    if (tid == 0) {\n",
    "        // Atomicly add to the final result location (for multi-block summation)\n",
    "        atomicAdd(d_out, sdata[0]);\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "int main() {\n",
    "    std::cout << \"--- Starting Neural Network Forward Pass on GPU ---\" << std::endl;\n",
    "    std::cout << \"M=\" << M << \", D_in=\" << DIN << \", D_hid=\" << DHID << \", C=\" << C << std::endl;\n",
    "\n",
    "    // --- 1. Host Memory Setup ---\n",
    "    std::vector<int> h_Y_true(M);\n",
    "    std::vector<float> h_X(M * DIN);\n",
    "    \n",
    "    // Initialize Input X (e.g., random data between 0 and 1)\n",
    "    for (int i = 0; i < M * DIN; ++i) {\n",
    "        h_X[i] = (float)rand() / RAND_MAX;\n",
    "    }\n",
    "\n",
    "    // Initialize True Labels Y_true (e.g., random class indices)\n",
    "    for (int i = 0; i < M; ++i) {\n",
    "        h_Y_true[i] = rand() % C;\n",
    "    }\n",
    "\n",
    "    // --- 2. Device Memory Allocation ---\n",
    "    float *d_X, *d_W1, *d_B1, *d_A1; // Layer 1\n",
    "    float *d_W2, *d_B2, *d_Z2;       // Layer 2 (Output)\n",
    "    int *d_Y_true;                   // Labels\n",
    "    float *d_loss_elements, *d_final_loss_sum; // Loss calculation\n",
    "\n",
    "    // Input and Labels\n",
    "    CUDA_CHECK(cudaMalloc((void**)&d_X, M * DIN * sizeof(float)));\n",
    "    CUDA_CHECK(cudaMalloc((void**)&d_Y_true, M * sizeof(int)));\n",
    "\n",
    "    // Layer 1 Tensors\n",
    "    CUDA_CHECK(cudaMalloc((void**)&d_W1, DIN * DHID * sizeof(float)));\n",
    "    CUDA_CHECK(cudaMalloc((void**)&d_B1, DHID * sizeof(float)));\n",
    "    CUDA_CHECK(cudaMalloc((void**)&d_A1, M * DHID * sizeof(float))); // A1 is Z1 after ReLU\n",
    "\n",
    "    // Layer 2 Tensors\n",
    "    CUDA_CHECK(cudaMalloc((void**)&d_W2, DHID * C * sizeof(float)));\n",
    "    CUDA_CHECK(cudaMalloc((void**)&d_B2, C * sizeof(float)));\n",
    "    CUDA_CHECK(cudaMalloc((void**)&d_Z2, M * C * sizeof(float))); // Z2 is Softmax input/output\n",
    "\n",
    "    // Loss Tensors\n",
    "    CUDA_CHECK(cudaMalloc((void**)&d_loss_elements, M * sizeof(float)));\n",
    "    CUDA_CHECK(cudaMalloc((void**)&d_final_loss_sum, sizeof(float)));\n",
    "    \n",
    "    // --- 3. Initial Transfers (Host -> Device) ---\n",
    "    CUDA_CHECK(cudaMemcpy(d_X, h_X.data(), M * DIN * sizeof(float), cudaMemcpyHostToDevice));\n",
    "    CUDA_CHECK(cudaMemcpy(d_Y_true, h_Y_true.data(), M * sizeof(int), cudaMemcpyHostToDevice));\n",
    "    \n",
    "    // Initialize final sum to 0\n",
    "    float zero = 0.0f;\n",
    "    CUDA_CHECK(cudaMemcpy(d_final_loss_sum, &zero, sizeof(float), cudaMemcpyHostToDevice));\n",
    "\n",
    "    // --- 4. Launch Initialization Kernels (Weights) ---\n",
    "    int blocks_W1 = DIV_UP(DIN * DHID, THREADS_PER_BLOCK);\n",
    "    int blocks_W2 = DIV_UP(DHID * C, THREADS_PER_BLOCK);\n",
    "    int blocks_B1 = DIV_UP(DHID, THREADS_PER_BLOCK);\n",
    "    int blocks_B2 = DIV_UP(C, THREADS_PER_BLOCK);\n",
    "\n",
    "    initWeightsKernel<<<blocks_W1, THREADS_PER_BLOCK>>>(d_W1, DIN * DHID);\n",
    "    initWeightsKernel<<<blocks_W2, THREADS_PER_BLOCK>>>(d_W2, DHID * C);\n",
    "    initWeightsKernel<<<blocks_B1, THREADS_PER_BLOCK>>>(d_B1, DHID);\n",
    "    initWeightsKernel<<<blocks_B2, THREADS_PER_BLOCK>>>(d_B2, C);\n",
    "        \n",
    "    // Setup grid for L1 GEMM (M x DHID output)\n",
    "    dim3 grid1(DIV_UP(DHID, 16), DIV_UP(M, 16)); // Simple 16x16 block decomposition\n",
    "    dim3 block1(16, 16);\n",
    "\n",
    "    std::cout << \"\\n[L1] 1. Executing GEMM + Bias (X * W1 + B1)...\" << std::endl;\n",
    "    // X (M x DIN) * W1 (DIN x DHID) -> A1 (M x DHID)\n",
    "    simpleGemmBiasKernel<<<grid1, block1>>>(d_X, d_W1, d_B1, d_A1, M, DIN, DHID);\n",
    "\n",
    "    // Setup grid for L1 ReLU (M * DHID elements)\n",
    "    int size1 = M * DHID;\n",
    "    int blocks_relu1 = DIV_UP(size1, THREADS_PER_BLOCK);\n",
    "\n",
    "    std::cout << \"[L1] 2. Executing ReLU Activation...\" << std::endl;\n",
    "    // ReLU(A1) -> A1 (in place)\n",
    "    reluKernel<<<blocks_relu1, THREADS_PER_BLOCK>>>(d_A1, size1);\n",
    "\n",
    "    // Setup grid for L2 GEMM (M x C output)\n",
    "    dim3 grid2(DIV_UP(C, 16), DIV_UP(M, 16));\n",
    "    dim3 block2(16, 16);\n",
    "\n",
    "    std::cout << \"[L2] 3. Executing GEMM + Bias (A1 * W2 + B2)...\" << std::endl;\n",
    "    simpleGemmBiasKernel<<<grid2, block2>>>(d_A1, d_W2, d_B2, d_Z2, M, DHID, C);\n",
    "\n",
    "    // Setup grid for Softmax (M rows, 1 block per row)\n",
    "    int blocks_softmax = DIV_UP(M, THREADS_PER_BLOCK);\n",
    "\n",
    "    std::cout << \"[L2] 4. Executing Softmax Activation...\" << std::endl;\n",
    "    // Softmax(Z2) -> Z2 (in place, now Y_hat)\n",
    "    softmaxKernel<<<blocks_softmax, THREADS_PER_BLOCK>>>(d_Z2, M, C);\n",
    "\n",
    "    // --- 7. LOSS CALCULATION ---\n",
    "\n",
    "    // Setup grid for Cross-Entropy (M samples)\n",
    "    int blocks_ce = DIV_UP(M, THREADS_PER_BLOCK);\n",
    "\n",
    "    std::cout << \"[Loss] 5. Executing Cross-Entropy (Element-wise Loss)...\" << std::endl;\n",
    "    // CrossEntropy(Y_hat, Y_true) -> d_loss_elements (M x 1)\n",
    "    crossEntropyKernel<<<blocks_ce, THREADS_PER_BLOCK>>>(d_Z2, d_Y_true, d_loss_elements, M, C);\n",
    "\n",
    "    // Setup grid for Final Reduction (M elements in d_loss_elements)\n",
    "    int size_loss = M;\n",
    "    int blocks_reduce = DIV_UP(size_loss, THREADS_PER_BLOCK);\n",
    "\n",
    "    std::cout << \"[Loss] 6. Executing Reduction (Summing Loss)...\" << std::endl;\n",
    "    // Sum d_loss_elements -> d_final_loss_sum (1 element)\n",
    "    reduceSumKernel<<<blocks_reduce, THREADS_PER_BLOCK>>>(d_loss_elements, d_final_loss_sum, size_loss);\n",
    "\n",
    "    // --- 8. Final Transfer and Verification ---\n",
    "    CUDA_CHECK(cudaDeviceSynchronize()); // Wait for ALL kernels to finish\n",
    "\n",
    "    float final_loss_sum;\n",
    "    CUDA_CHECK(cudaMemcpy(&final_loss_sum, d_final_loss_sum, sizeof(float), cudaMemcpyDeviceToHost));\n",
    "    \n",
    "    float avg_loss = final_loss_sum / (float)M;\n",
    "\n",
    "    std::cout << \"\\n--- Forward Pass Complete ---\" << std::endl;\n",
    "    std::cout << \"Total Loss Sum: \" << final_loss_sum << std::endl;\n",
    "    std::cout << \"Average Cross-Entropy Loss (Final Output): \" << avg_loss << std::endl;\n",
    "    \n",
    "    // --- 9. Cleanup ---\n",
    "    cudaFree(d_X);\n",
    "    cudaFree(d_Y_true);\n",
    "    cudaFree(d_W1); cudaFree(d_B1); cudaFree(d_A1);\n",
    "    cudaFree(d_W2); cudaFree(d_B2); cudaFree(d_Z2);\n",
    "    cudaFree(d_loss_elements);\n",
    "    cudaFree(d_final_loss_sum);\n",
    "\n",
    "    return 0;\n",
    "}\n",
    "EOF\n",
    "\n",
    "nvcc -std=c++17 forward_pass.cu -o forward_pass_output && ./forward_pass_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7ba39209-f2e4-40e6-9018-9a2332ca5908",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "backward_pass.cu(181): warning #177-D: variable \"blocks_bias_hid\" was declared but never referenced\n",
      "\n",
      "Remark: The warnings can be suppressed with \"-diag-suppress <warning-number>\"\n",
      "\n",
      "backward_pass.cu(11): warning #177-D: variable \"LR\" was declared but never referenced\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Backward Pass Kernel Demonstration ---\n",
      "Simulating data flow for M=128, D_hid=32, C=10\n",
      "\n",
      "[L2] 1. Initial Gradient (dL/dZ2 = Y_hat - Y_true)...\n",
      "[L2] 2. Bias Gradient (dL/dB2)...\n",
      "[L2] 3. Weight Gradient (dL/dW2)...\n",
      "[L2] 4. Input Gradient (dL/dA1) to be passed to L1...\n",
      "[L1] 5. ReLU Backward (dL/dZ1 = dL/dA1 * ReLU'(Z1_pre))...\n",
      "\n",
      "[Optimizer] Update kernels (W = W - LR * dW) are ready.\n",
      "\n",
      "--- Backward Pass Kernels executed successfully. ---\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "export CUDA_HOME=/packages/apps/spack/21/opt/spack/linux-rocky8-zen3/gcc-12.1.0/cuda-12.0.1-x3bnvayrybncl3rqu6zk4zzu4oztblqi\n",
    "export PATH=$CUDA_HOME/bin:$PATH\n",
    "export LD_LIBRARY_PATH=$CUDA_HOME/lib64:$LD_LIBRARY_PATH\n",
    "cat > backward_pass.cu <<'EOF'\n",
    "#include <iostream>\n",
    "#include <vector>\n",
    "#include <cmath>\n",
    "#include <numeric>\n",
    "\n",
    "// Global constants for the network size (needed for memory allocation and loop bounds)\n",
    "const int M = 128;   // Batch Size\n",
    "const int DIN = 64;  // Input Dimension\n",
    "const int DHID = 32; // Hidden Dimension\n",
    "const int C = 10;    // Number of Classes\n",
    "const float LR = 0.01f; // Learning Rate\n",
    "\n",
    "const int THREADS_PER_BLOCK = 256;\n",
    "\n",
    "// Helper macros\n",
    "#define CUDA_CHECK(call)                                                          \\\n",
    "{                                                                                 \\\n",
    "    cudaError_t err = call;                                                       \\\n",
    "    if (err != cudaSuccess) {                                                     \\\n",
    "        fprintf(stderr, \"CUDA error at %s:%d code=%d(%s) \\\"%s\\\"\\n\",               \\\n",
    "                __FILE__, __LINE__, err, cudaGetErrorString(err), #call);         \\\n",
    "        exit(EXIT_FAILURE);                                                       \\\n",
    "    }                                                                             \\\n",
    "}\n",
    "#define DIV_UP(a, b) ((a + b - 1) / b)\n",
    "\n",
    "// --- BACKWARD PASS KERNELS ---\n",
    "\n",
    "/**\n",
    " * @brief Calculates the initial gradient dL/dZ2 (pre-softmax/logit).\n",
    " * dL/dZ2 = Y_hat - Y_true\n",
    " * * @param Y_hat Softmax output (M x C)\n",
    " * @param Y_true True labels (M x 1)\n",
    " * @param d_dZ2 Output gradient (M x C)\n",
    " */\n",
    "__global__ void lossBackwardKernel(const float *Y_hat, const int *Y_true, float *d_dZ2, \n",
    "                                   int M, int C) {\n",
    "    int row = blockIdx.y * blockDim.y + threadIdx.y;\n",
    "    int col = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "\n",
    "    if (row < M && col < C) {\n",
    "        float gradient = Y_hat[row * C + col]; // Start with P_i\n",
    "        \n",
    "        // If this column is the true class index, subtract 1\n",
    "        if (col == Y_true[row]) {\n",
    "            gradient -= 1.0f;\n",
    "        }\n",
    "\n",
    "        d_dZ2[row * C + col] = gradient / (float)M; // Normalize by Batch Size (M)\n",
    "    }\n",
    "}\n",
    "\n",
    "/**\n",
    " * @brief Performs the backward pass for Matrix Multiplication: dL/dX = dL/dZ * W^T\n",
    " * Calculates the gradient passed back to the previous layer.\n",
    " */\n",
    "__global__ void gemmBackwardInputKernel(const float *d_dZ, const float *W, float *d_dX, \n",
    "                                        int M, int K, int N) {\n",
    "    // dL/dX is (M x K)\n",
    "    int row = blockIdx.y * blockDim.y + threadIdx.y;\n",
    "    int col = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "\n",
    "    if (row < M && col < K) {\n",
    "        float sum = 0.0f;\n",
    "        // Dot product of dZ[row, :] and W^T[:, col]\n",
    "        for (int n = 0; n < N; ++n) {\n",
    "            // W is (K x N), W^T access is W[k*N + n] -> W[col*N + n]\n",
    "            sum += d_dZ[row * N + n] * W[col * N + n];\n",
    "        }\n",
    "        d_dX[row * K + col] = sum;\n",
    "    }\n",
    "}\n",
    "\n",
    "/**\n",
    " * @brief Performs the backward pass for Matrix Multiplication: dL/dW = X^T * dL/dZ\n",
    " * Calculates the gradient of the weights.\n",
    " */\n",
    "__global__ void gemmBackwardWeightKernel(const float *X, const float *d_dZ, float *d_dW, \n",
    "                                         int M, int K, int N) {\n",
    "    // dL/dW is (K x N)\n",
    "    int row = blockIdx.y * blockDim.y + threadIdx.y; // K rows\n",
    "    int col = blockIdx.x * blockDim.x + threadIdx.x; // N cols\n",
    "\n",
    "    if (row < K && col < N) {\n",
    "        float sum = 0.0f;\n",
    "        // Dot product of X^T[row, :] and dZ[:, col]\n",
    "        for (int m = 0; m < M; ++m) {\n",
    "            // X[m * K + row] * d_dZ[m * N + col]\n",
    "            sum += X[m * K + row] * d_dZ[m * N + col];\n",
    "        }\n",
    "        d_dW[row * N + col] = sum;\n",
    "    }\n",
    "}\n",
    "\n",
    "/**\n",
    " * @brief Performs the backward pass for Bias: dL/dB = sum(dL/dZ, axis=0)\n",
    " */\n",
    "__global__ void biasBackwardKernel(const float *d_dZ, float *d_dB, int M, int N) {\n",
    "    // One thread per column (bias element)\n",
    "    int col = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "\n",
    "    if (col < N) {\n",
    "        float sum = 0.0f;\n",
    "        // Sum down the column\n",
    "        for (int m = 0; m < M; ++m) {\n",
    "            sum += d_dZ[m * N + col];\n",
    "        }\n",
    "        d_dB[col] = sum;\n",
    "    }\n",
    "}\n",
    "\n",
    "/**\n",
    " * @brief Performs element-wise ReLU backward: dL/dZ = dL/dA * ReLU'(Z)\n",
    " * ReLU'(Z) = 1 if Z > 0, 0 otherwise.\n",
    " */\n",
    "__global__ void reluBackwardKernel(float *d_dZ, const float *Z, int size) {\n",
    "    int i = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (i < size) {\n",
    "        // Apply the mask: gradient is zeroed out where the pre-activation was <= 0\n",
    "        if (Z[i] <= 0.0f) {\n",
    "            d_dZ[i] = 0.0f;\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "/**\n",
    " * @brief Performs Stochastic Gradient Descent (SGD) update: W = W - LR * dW\n",
    " */\n",
    "__global__ void updateWeightsKernel(float *W, const float *dW, int size, float lr) {\n",
    "    int i = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (i < size) {\n",
    "        W[i] -= lr * dW[i];\n",
    "    }\n",
    "}\n",
    "\n",
    "// --- MAIN FUNCTION (Simulating the Backward Pass Data Flow) ---\n",
    "\n",
    "int main() {\n",
    "    std::cout << \"--- Backward Pass Kernel Demonstration ---\" << std::endl;\n",
    "    std::cout << \"Simulating data flow for M=\" << M << \", D_hid=\" << DHID << \", C=\" << C << std::endl;\n",
    "\n",
    "    // --- 1. Memory Allocation and Dummy Data (Simulate Forward Pass Outputs) ---\n",
    "    float *d_W2, *d_B2, *d_A1, *d_Z1_pre, *d_dZ2_grad;\n",
    "    int *d_Y_true;\n",
    "\n",
    "    // Layer 2 Tensors (Required for L2 Backward)\n",
    "    CUDA_CHECK(cudaMalloc((void**)&d_W2, DHID * C * sizeof(float)));\n",
    "    CUDA_CHECK(cudaMalloc((void**)&d_B2, C * sizeof(float)));\n",
    "    CUDA_CHECK(cudaMalloc((void**)&d_dZ2_grad, M * C * sizeof(float))); // Input gradient\n",
    "\n",
    "    // Layer 1 Tensors (Required for L1 Backward)\n",
    "    CUDA_CHECK(cudaMalloc((void**)&d_A1, M * DHID * sizeof(float))); // Input to L2 GEMM\n",
    "    CUDA_CHECK(cudaMalloc((void**)&d_Z1_pre, M * DHID * sizeof(float))); // L1 Pre-ReLU\n",
    "    \n",
    "    // Gradients to be calculated\n",
    "    float *d_dW2, *d_dB2, *d_dA1_grad, *d_dW1, *d_dB1;\n",
    "    CUDA_CHECK(cudaMalloc((void**)&d_dW2, DHID * C * sizeof(float)));\n",
    "    CUDA_CHECK(cudaMalloc((void**)&d_dB2, C * sizeof(float)));\n",
    "    CUDA_CHECK(cudaMalloc((void**)&d_dA1_grad, M * DHID * sizeof(float)));\n",
    "    CUDA_CHECK(cudaMalloc((void**)&d_dW1, DIN * DHID * sizeof(float)));\n",
    "    CUDA_CHECK(cudaMalloc((void**)&d_dB1, DHID * sizeof(float)));\n",
    "\n",
    "    // Dummy Initialization (Fill with arbitrary values to simulate forward pass)\n",
    "    // In a real scenario, these would be outputs of the forward pass\n",
    "    std::vector<int> h_Y_true(M);\n",
    "    std::vector<float> h_Y_hat(M * C);\n",
    "    for (int i = 0; i < M * C; ++i) h_Y_hat[i] = (float)rand() / RAND_MAX;\n",
    "    for (int i = 0; i < M; ++i) h_Y_true[i] = rand() % C;\n",
    "    CUDA_CHECK(cudaMalloc((void**)&d_Y_true, M * sizeof(int)));\n",
    "    CUDA_CHECK(cudaMemcpy(d_Y_true, h_Y_true.data(), M * sizeof(int), cudaMemcpyHostToDevice));\n",
    "    // Y_hat is copied into d_dZ2_grad to simulate the first step\n",
    "    CUDA_CHECK(cudaMemcpy(d_dZ2_grad, h_Y_hat.data(), M * C * sizeof(float), cudaMemcpyHostToDevice));\n",
    "    \n",
    "    // --- 2. BACKWARD PASS CHAIN EXECUTION (Simplified) ---\n",
    "    \n",
    "    // Grid/Block Setup\n",
    "    dim3 grid_mat(DIV_UP(C, 16), DIV_UP(M, 16));\n",
    "    dim3 block_mat(16, 16);\n",
    "    int threads_bias = THREADS_PER_BLOCK;\n",
    "    int blocks_bias_c = DIV_UP(C, THREADS_PER_BLOCK);\n",
    "    int blocks_bias_hid = DIV_UP(DHID, THREADS_PER_BLOCK);\n",
    "    \n",
    "    std::cout << \"\\n[L2] 1. Initial Gradient (dL/dZ2 = Y_hat - Y_true)...\" << std::endl;\n",
    "    // d_dZ2_grad is updated in-place\n",
    "    lossBackwardKernel<<<grid_mat, block_mat>>>(d_dZ2_grad, d_Y_true, d_dZ2_grad, M, C);\n",
    "\n",
    "    std::cout << \"[L2] 2. Bias Gradient (dL/dB2)...\" << std::endl;\n",
    "    biasBackwardKernel<<<blocks_bias_c, threads_bias>>>(d_dZ2_grad, d_dB2, M, C);\n",
    "\n",
    "    std::cout << \"[L2] 3. Weight Gradient (dL/dW2)...\" << std::endl;\n",
    "    dim3 grid_w2(DIV_UP(C, 16), DIV_UP(DHID, 16));\n",
    "    gemmBackwardWeightKernel<<<grid_w2, block_mat>>>(d_A1, d_dZ2_grad, d_dW2, M, DHID, C);\n",
    "\n",
    "    std::cout << \"[L2] 4. Input Gradient (dL/dA1) to be passed to L1...\" << std::endl;\n",
    "    dim3 grid_a1(DIV_UP(DHID, 16), DIV_UP(M, 16));\n",
    "    gemmBackwardInputKernel<<<grid_a1, block_mat>>>(d_dZ2_grad, d_W2, d_dA1_grad, M, DHID, C);\n",
    "\n",
    "    // Continue to L1 (just showing the start of the chain)\n",
    "\n",
    "    std::cout << \"[L1] 5. ReLU Backward (dL/dZ1 = dL/dA1 * ReLU'(Z1_pre))...\" << std::endl;\n",
    "    int size1 = M * DHID;\n",
    "    reluBackwardKernel<<<DIV_UP(size1, THREADS_PER_BLOCK), THREADS_PER_BLOCK>>>(d_dA1_grad, d_Z1_pre, size1);\n",
    "\n",
    "    std::cout << \"\\n[Optimizer] Update kernels (W = W - LR * dW) are ready.\" << std::endl;\n",
    "\n",
    "    CUDA_CHECK(cudaDeviceSynchronize());\n",
    "    std::cout << \"\\n--- Backward Pass Kernels executed successfully. ---\" << std::endl;\n",
    "\n",
    "    // Cleanup\n",
    "    cudaFree(d_W2); cudaFree(d_B2); cudaFree(d_A1); cudaFree(d_Z1_pre); cudaFree(d_dZ2_grad);\n",
    "    cudaFree(d_dW2); cudaFree(d_dB2); cudaFree(d_dA1_grad); cudaFree(d_dW1); cudaFree(d_dB1);\n",
    "    cudaFree(d_Y_true);\n",
    "\n",
    "    return 0;\n",
    "}\n",
    "EOF\n",
    "\n",
    "nvcc -std=c++17 backward_pass.cu -o backward_pass_output && ./backward_pass_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3ccd050c-9b2f-48ec-b236-5ebf912f464d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Integrated Full Training Step on GPU (3 Layers) ---\n",
      "Configuration: M=128, D_in=64, D_hid=32, C=10, LR=0.01\n",
      "\n",
      "--- FORWARD PASS ---\n",
      "\n",
      "--- BACKWARD PASS ---\n",
      "\n",
      "--- OPTIMIZER STEP (SGD) ---\n",
      "\n",
      "--- Training Step Complete ---\n",
      "Average Cross-Entropy Loss: 2.30259\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "export CUDA_HOME=/packages/apps/spack/21/opt/spack/linux-rocky8-zen3/gcc-12.1.0/cuda-12.0.1-x3bnvayrybncl3rqu6zk4zzu4oztblqi\n",
    "export PATH=$CUDA_HOME/bin:$PATH\n",
    "export LD_LIBRARY_PATH=$CUDA_HOME/lib64:$LD_LIBRARY_PATH\n",
    "cat > full_loop.cu <<'EOF'\n",
    "\n",
    "#include <iostream>\n",
    "#include <vector>\n",
    "#include <cmath>\n",
    "#include <numeric>\n",
    "\n",
    "// Global constants for the network size\n",
    "const int M = 128;   // Batch Size (Rows)\n",
    "const int DIN = 64;  // Input Dimension\n",
    "const int DHID = 32; // Hidden Dimension Size (used for both hidden layers)\n",
    "const int C = 10;    // Number of Classes (Output Dimension)\n",
    "const float LR = 0.01f; // Learning Rate\n",
    "\n",
    "const int THREADS_PER_BLOCK = 256;\n",
    "\n",
    "// Helper macros\n",
    "#define CUDA_CHECK(call)                                                          \\\n",
    "{                                                                                 \\\n",
    "    cudaError_t err = call;                                                       \\\n",
    "    if (err != cudaSuccess) {                                                     \\\n",
    "        fprintf(stderr, \"CUDA error at %s:%d code=%d(%s) \\\"%s\\\"\\n\",               \\\n",
    "                __FILE__, __LINE__, err, cudaGetErrorString(err), #call);         \\\n",
    "        exit(EXIT_FAILURE);                                                       \\\n",
    "    }                                                                             \\\n",
    "}\n",
    "#define DIV_UP(a, b) ((a + b - 1) / b)\n",
    "\n",
    "// --- UTILITY KERNELS ---\n",
    "\n",
    "/**\n",
    " * @brief Initializes a device vector with random numbers between -0.5 and 0.5.\n",
    " */\n",
    "__global__ void initWeightsKernel(float *d_data, int size) {\n",
    "    int i = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (i < size) {\n",
    "        unsigned int seed = i;\n",
    "        float rand_val = (float)seed / (float)0xFFFFFFFF; \n",
    "        d_data[i] = (rand_val - 0.5f); \n",
    "    }\n",
    "}\n",
    "\n",
    "/**\n",
    " * @brief Performs a generic parallel sum reduction.\n",
    " */\n",
    "__global__ void reduceSumKernel(float *d_in, float *d_out, int size) {\n",
    "    __shared__ float sdata[THREADS_PER_BLOCK];\n",
    "    \n",
    "    int tid = threadIdx.x;\n",
    "    int i = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    \n",
    "    sdata[tid] = (i < size) ? d_in[i] : 0.0f;\n",
    "    __syncthreads();\n",
    "\n",
    "    for (int s = blockDim.x / 2; s > 0; s >>= 1) {\n",
    "        if (tid < s) {\n",
    "            sdata[tid] += sdata[tid + s];\n",
    "        }\n",
    "        __syncthreads();\n",
    "    }\n",
    "    \n",
    "    if (tid == 0) {\n",
    "        atomicAdd(d_out, sdata[0]); \n",
    "    }\n",
    "}\n",
    "\n",
    "// --- FORWARD PASS KERNELS ---\n",
    "\n",
    "/**\n",
    " * @brief Performs Matrix Multiplication and Bias Addition: Z = X * W + B.\n",
    " */\n",
    "__global__ void simpleGemmBiasKernel(const float *X, const float *W, const float *B, float *Z, \n",
    "                                     int M, int K, int N) {\n",
    "    int row = blockIdx.y * blockDim.y + threadIdx.y;\n",
    "    int col = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "\n",
    "    if (row < M && col < N) {\n",
    "        float sum = 0.0f;\n",
    "        for (int k = 0; k < K; ++k) {\n",
    "            sum += X[row * K + k] * W[k * N + col];\n",
    "        }\n",
    "        sum += B[col];\n",
    "        Z[row * N + col] = sum;\n",
    "    }\n",
    "}\n",
    "\n",
    "/**\n",
    " * @brief Performs element-wise ReLU activation: A = max(0, Z).\n",
    " */\n",
    "__global__ void reluKernel(float *A, int size) {\n",
    "    int i = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (i < size) {\n",
    "        if (A[i] < 0.0f) {\n",
    "            A[i] = 0.0f;\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "/**\n",
    " * @brief Performs row-wise Softmax activation.\n",
    " */\n",
    "__global__ void softmaxKernel(float *Z, int M, int C) {\n",
    "    int row = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "\n",
    "    if (row < M) {\n",
    "        float max_val = -1e20f;\n",
    "        float sum_exp = 0.0f;\n",
    "        int row_start = row * C;\n",
    "        \n",
    "        for (int j = 0; j < C; ++j) {\n",
    "            max_val = fmaxf(max_val, Z[row_start + j]);\n",
    "        }\n",
    "\n",
    "        for (int j = 0; j < C; ++j) {\n",
    "            Z[row_start + j] = expf(Z[row_start + j] - max_val);\n",
    "            sum_exp += Z[row_start + j];\n",
    "        }\n",
    "\n",
    "        for (int j = 0; j < C; ++j) {\n",
    "            Z[row_start + j] /= sum_exp;\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "/**\n",
    " * @brief Calculates the Cross-Entropy Loss element-wise.\n",
    " */\n",
    "__global__ void crossEntropyKernel(const float *Y_hat, const int *Y_true, float *d_loss_elements, \n",
    "                                   int M, int C) {\n",
    "    int i = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "\n",
    "    if (i < M) {\n",
    "        int true_class = Y_true[i];\n",
    "        float predicted_prob = Y_hat[i * C + true_class];\n",
    "        float loss = -logf(fmaxf(predicted_prob, 1e-9f)); \n",
    "        d_loss_elements[i] = loss;\n",
    "    }\n",
    "}\n",
    "\n",
    "// --- BACKWARD PASS KERNELS (REUSED GENERIC FUNCTIONS) ---\n",
    "\n",
    "/**\n",
    " * @brief Calculates the initial gradient dL/dZ (Softmax + Cross-Entropy fusion).\n",
    " */\n",
    "__global__ void lossBackwardKernel(const float *Y_hat, const int *Y_true, float *d_dZ_out, \n",
    "                                   int M, int C) {\n",
    "    int row = blockIdx.y * blockDim.y + threadIdx.y;\n",
    "    int col = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "\n",
    "    if (row < M && col < C) {\n",
    "        float gradient = Y_hat[row * C + col]; \n",
    "        if (col == Y_true[row]) {\n",
    "            gradient -= 1.0f;\n",
    "        }\n",
    "        // Normalize by Batch Size (M)\n",
    "        d_dZ_out[row * C + col] = gradient / (float)M; \n",
    "    }\n",
    "}\n",
    "\n",
    "/**\n",
    " * @brief Calculates the gradient passed to the previous layer: dL/dX = dL/dZ * W^T\n",
    " */\n",
    "__global__ void gemmBackwardInputKernel(const float *d_dZ, const float *W, float *d_dX, \n",
    "                                        int M, int K, int N) {\n",
    "    // d_dX dimensions: M x K\n",
    "    // d_dZ dimensions: M x N\n",
    "    // W dimensions: K x N\n",
    "    int row = blockIdx.y * blockDim.y + threadIdx.y;\n",
    "    int col = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "\n",
    "    if (row < M && col < K) {\n",
    "        float sum = 0.0f;\n",
    "        for (int n = 0; n < N; ++n) {\n",
    "            // W[col * N + n] performs implicit transpose on W (originally K x N)\n",
    "            sum += d_dZ[row * N + n] * W[col * N + n];\n",
    "        }\n",
    "        d_dX[row * K + col] = sum;\n",
    "    }\n",
    "}\n",
    "\n",
    "/**\n",
    " * @brief Calculates the weight gradient: dL/dW = X^T * dL/dZ\n",
    " */\n",
    "__global__ void gemmBackwardWeightKernel(const float *X, const float *d_dZ, float *d_dW, \n",
    "                                         int M, int K, int N) {\n",
    "    // d_dW dimensions: K x N\n",
    "    // X dimensions: M x K\n",
    "    // d_dZ dimensions: M x N\n",
    "    int row = blockIdx.y * blockDim.y + threadIdx.y;\n",
    "    int col = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "\n",
    "    if (row < K && col < N) {\n",
    "        float sum = 0.0f;\n",
    "        for (int m = 0; m < M; ++m) {\n",
    "            // X[m * K + row] performs implicit transpose on X (originally M x K)\n",
    "            sum += X[m * K + row] * d_dZ[m * N + col];\n",
    "        }\n",
    "        d_dW[row * N + col] = sum;\n",
    "    }\n",
    "}\n",
    "\n",
    "/**\n",
    " * @brief Performs the backward pass for Bias: dL/dB = sum(dL/dZ, axis=0)\n",
    " */\n",
    "__global__ void biasBackwardKernel(const float *d_dZ, float *d_dB, int M, int N) {\n",
    "    int col = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "\n",
    "    if (col < N) {\n",
    "        float sum = 0.0f;\n",
    "        for (int m = 0; m < M; ++m) {\n",
    "            sum += d_dZ[m * N + col];\n",
    "        }\n",
    "        d_dB[col] = sum;\n",
    "    }\n",
    "}\n",
    "\n",
    "/**\n",
    " * @brief Performs element-wise ReLU backward: dL/dZ = dL/dA * ReLU'(Z)\n",
    " */\n",
    "__global__ void reluBackwardKernel(float *d_dZ, const float *Z, int size) {\n",
    "    int i = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (i < size) {\n",
    "        // Sets gradient to zero where the forward pass pre-activation was <= 0\n",
    "        if (Z[i] <= 0.0f) {\n",
    "            d_dZ[i] = 0.0f;\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "/**\n",
    " * @brief Performs Stochastic Gradient Descent (SGD) update: W = W - LR * dW\n",
    " */\n",
    "__global__ void updateWeightsKernel(float *W, const float *dW, int size, float lr) {\n",
    "    int i = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (i < size) {\n",
    "        W[i] -= lr * dW[i];\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "// --- MAIN FUNCTION: Orchestrating the Full Network ---\n",
    "\n",
    "int main() {\n",
    "    std::cout << \"--- Integrated Full Training Step on GPU (3 Layers) ---\" << std::endl;\n",
    "    std::cout << \"Configuration: M=\" << M << \", D_in=\" << DIN << \", D_hid=\" << DHID << \", C=\" << C << \", LR=\" << LR << std::endl;\n",
    "\n",
    "    // --- 1. Memory Setup and Allocation (3 LAYERS) ---\n",
    "    // Layer 1: DIN -> DHID\n",
    "    float *d_W1, *d_B1, *d_Z1_pre, *d_A1; \n",
    "    // Layer 2: DHID -> DHID\n",
    "    float *d_W2, *d_B2, *d_Z2_pre, *d_A2; \n",
    "    // Layer 3 (Output): DHID -> C\n",
    "    float *d_W3, *d_B3, *d_Z3; // d_Z3 holds Y_hat after Softmax\n",
    "    \n",
    "    float *d_X;\n",
    "    int *d_Y_true;\n",
    "    float *d_loss_elements, *d_final_loss_sum; \n",
    "    \n",
    "    // Gradients\n",
    "    float *d_dW1, *d_dB1, *d_dA1_grad;\n",
    "    float *d_dW2, *d_dB2, *d_dA2_grad;\n",
    "    float *d_dW3, *d_dB3, *d_dZ3_grad;\n",
    "\n",
    "    std::vector<int> h_Y_true(M);\n",
    "    std::vector<float> h_X(M * DIN);\n",
    "    for (int i = 0; i < M * DIN; ++i) h_X[i] = (float)rand() / RAND_MAX;\n",
    "    for (int i = 0; i < M; ++i) h_Y_true[i] = rand() % C;\n",
    "    \n",
    "    // --- Allocate Tensors ---\n",
    "    CUDA_CHECK(cudaMalloc((void**)&d_X, M * DIN * sizeof(float)));\n",
    "    CUDA_CHECK(cudaMalloc((void**)&d_Y_true, M * sizeof(int)));\n",
    "    \n",
    "    // L1 Tensors (DIN -> DHID)\n",
    "    CUDA_CHECK(cudaMalloc((void**)&d_W1, DIN * DHID * sizeof(float)));\n",
    "    CUDA_CHECK(cudaMalloc((void**)&d_B1, DHID * sizeof(float)));\n",
    "    CUDA_CHECK(cudaMalloc((void**)&d_Z1_pre, M * DHID * sizeof(float))); \n",
    "    CUDA_CHECK(cudaMalloc((void**)&d_A1, M * DHID * sizeof(float)));\n",
    "    CUDA_CHECK(cudaMalloc((void**)&d_dW1, DIN * DHID * sizeof(float)));\n",
    "    CUDA_CHECK(cudaMalloc((void**)&d_dB1, DHID * sizeof(float)));\n",
    "    CUDA_CHECK(cudaMalloc((void**)&d_dA1_grad, M * DHID * sizeof(float))); // dL/dA1, reused as dL/dZ1\n",
    "    \n",
    "    // L2 Tensors (DHID -> DHID)\n",
    "    CUDA_CHECK(cudaMalloc((void**)&d_W2, DHID * DHID * sizeof(float)));\n",
    "    CUDA_CHECK(cudaMalloc((void**)&d_B2, DHID * sizeof(float)));\n",
    "    CUDA_CHECK(cudaMalloc((void**)&d_Z2_pre, M * DHID * sizeof(float))); \n",
    "    CUDA_CHECK(cudaMalloc((void**)&d_A2, M * DHID * sizeof(float)));\n",
    "    CUDA_CHECK(cudaMalloc((void**)&d_dW2, DHID * DHID * sizeof(float)));\n",
    "    CUDA_CHECK(cudaMalloc((void**)&d_dB2, DHID * sizeof(float)));\n",
    "    CUDA_CHECK(cudaMalloc((void**)&d_dA2_grad, M * DHID * sizeof(float))); // dL/dA2, reused as dL/dZ2\n",
    "\n",
    "    // L3 Tensors (DHID -> C)\n",
    "    CUDA_CHECK(cudaMalloc((void**)&d_W3, DHID * C * sizeof(float)));\n",
    "    CUDA_CHECK(cudaMalloc((void**)&d_B3, C * sizeof(float)));\n",
    "    CUDA_CHECK(cudaMalloc((void**)&d_Z3, M * C * sizeof(float))); \n",
    "    CUDA_CHECK(cudaMalloc((void**)&d_dW3, DHID * C * sizeof(float)));\n",
    "    CUDA_CHECK(cudaMalloc((void**)&d_dB3, C * sizeof(float)));\n",
    "    CUDA_CHECK(cudaMalloc((void**)&d_dZ3_grad, M * C * sizeof(float)));\n",
    "\n",
    "    // Loss Tensors\n",
    "    CUDA_CHECK(cudaMalloc((void**)&d_loss_elements, M * sizeof(float)));\n",
    "    CUDA_CHECK(cudaMalloc((void**)&d_final_loss_sum, sizeof(float)));\n",
    "\n",
    "    // Initial Data Transfer and Initialization\n",
    "    CUDA_CHECK(cudaMemcpy(d_X, h_X.data(), M * DIN * sizeof(float), cudaMemcpyHostToDevice));\n",
    "    CUDA_CHECK(cudaMemcpy(d_Y_true, h_Y_true.data(), M * sizeof(int), cudaMemcpyHostToDevice));\n",
    "    float zero = 0.0f;\n",
    "    CUDA_CHECK(cudaMemcpy(d_final_loss_sum, &zero, sizeof(float), cudaMemcpyHostToDevice));\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    // Initialize Weights\n",
    "    initWeightsKernel<<<DIV_UP(DIN * DHID, THREADS_PER_BLOCK), THREADS_PER_BLOCK>>>(d_W1, DIN * DHID);\n",
    "    initWeightsKernel<<<DIV_UP(DHID, THREADS_PER_BLOCK), THREADS_PER_BLOCK>>>(d_B1, DHID);\n",
    "    initWeightsKernel<<<DIV_UP(DHID * DHID, THREADS_PER_BLOCK), THREADS_PER_BLOCK>>>(d_W2, DHID * DHID);\n",
    "    initWeightsKernel<<<DIV_UP(DHID, THREADS_PER_BLOCK), THREADS_PER_BLOCK>>>(d_B2, DHID);\n",
    "    initWeightsKernel<<<DIV_UP(DHID * C, THREADS_PER_BLOCK), THREADS_PER_BLOCK>>>(d_W3, DHID * C);\n",
    "    initWeightsKernel<<<DIV_UP(C, THREADS_PER_BLOCK), THREADS_PER_BLOCK>>>(d_B3, C);\n",
    "    \n",
    "    // --- 2. FORWARD PASS EXECUTION (3 LAYERS) ---\n",
    "    std::cout << \"\\n--- FORWARD PASS ---\" << std::endl;\n",
    "    dim3 block_mat(16, 16);\n",
    "    int size_hid = M * DHID;\n",
    "    \n",
    "    // --- L1: Input (DIN) -> Hidden (DHID) ---\n",
    "    dim3 grid_mat1(DIV_UP(DHID, 16), DIV_UP(M, 16));\n",
    "    simpleGemmBiasKernel<<<grid_mat1, block_mat>>>(d_X, d_W1, d_B1, d_Z1_pre, M, DIN, DHID);\n",
    "    CUDA_CHECK(cudaMemcpy(d_A1, d_Z1_pre, size_hid * sizeof(float), cudaMemcpyDeviceToDevice));\n",
    "    reluKernel<<<DIV_UP(size_hid, THREADS_PER_BLOCK), THREADS_PER_BLOCK>>>(d_A1, size_hid);\n",
    "\n",
    "    // --- L2: Hidden (DHID) -> Hidden (DHID) ---\n",
    "    // Kernel uses DHID as K and N\n",
    "    simpleGemmBiasKernel<<<grid_mat1, block_mat>>>(d_A1, d_W2, d_B2, d_Z2_pre, M, DHID, DHID);\n",
    "    CUDA_CHECK(cudaMemcpy(d_A2, d_Z2_pre, size_hid * sizeof(float), cudaMemcpyDeviceToDevice));\n",
    "    reluKernel<<<DIV_UP(size_hid, THREADS_PER_BLOCK), THREADS_PER_BLOCK>>>(d_A2, size_hid);\n",
    "\n",
    "    // --- L3: Hidden (DHID) -> Output (C) ---\n",
    "    dim3 grid_mat3(DIV_UP(C, 16), DIV_UP(M, 16));\n",
    "    simpleGemmBiasKernel<<<grid_mat3, block_mat>>>(d_A2, d_W3, d_B3, d_Z3, M, DHID, C);\n",
    "    softmaxKernel<<<DIV_UP(M, THREADS_PER_BLOCK), THREADS_PER_BLOCK>>>(d_Z3, M, C); // d_Z3 is now Y_hat\n",
    "\n",
    "    // LOSS CALCULATION\n",
    "    crossEntropyKernel<<<DIV_UP(M, THREADS_PER_BLOCK), THREADS_PER_BLOCK>>>(d_Z3, d_Y_true, d_loss_elements, M, C);\n",
    "    reduceSumKernel<<<DIV_UP(M, THREADS_PER_BLOCK), THREADS_PER_BLOCK>>>(d_loss_elements, d_final_loss_sum, M);\n",
    "\n",
    "    // --- 3. BACKWARD PASS EXECUTION (3 LAYERS) ---\n",
    "    std::cout << \"\\n--- BACKWARD PASS ---\" << std::endl;\n",
    "\n",
    "    // --- L3 BACKWARD (Output Layer: DHID -> C) ---\n",
    "    // Step 1: Calculate dL/dZ3 = Y_hat - Y_true\n",
    "    lossBackwardKernel<<<grid_mat3, block_mat>>>(d_Z3, d_Y_true, d_dZ3_grad, M, C);\n",
    "\n",
    "    // Step 2: Calculate dW3 and dB3 (Inputs: A2, dZ3_grad)\n",
    "    biasBackwardKernel<<<DIV_UP(C, THREADS_PER_BLOCK), THREADS_PER_BLOCK>>>(d_dZ3_grad, d_dB3, M, C);\n",
    "    dim3 grid_w3(DIV_UP(C, 16), DIV_UP(DHID, 16));\n",
    "    gemmBackwardWeightKernel<<<grid_w3, block_mat>>>(d_A2, d_dZ3_grad, d_dW3, M, DHID, C); // X=A2, K=DHID, N=C\n",
    "\n",
    "    // Step 3: Calculate dL/dA2 (gradient to pass to L2)\n",
    "    dim3 grid_a2(DIV_UP(DHID, 16), DIV_UP(M, 16));\n",
    "    gemmBackwardInputKernel<<<grid_a2, block_mat>>>(d_dZ3_grad, d_W3, d_dA2_grad, M, DHID, C); // K=DHID, N=C\n",
    "\n",
    "    // --- L2 BACKWARD (Hidden Layer: DHID -> DHID) ---\n",
    "    // Step 4: ReLU Backward on dL/dA2 to get dL/dZ2 in-place (d_dA2_grad now holds dL/dZ2)\n",
    "    reluBackwardKernel<<<DIV_UP(size_hid, THREADS_PER_BLOCK), THREADS_PER_BLOCK>>>(d_dA2_grad, d_Z2_pre, size_hid); \n",
    "\n",
    "    // Step 5: Calculate dW2 and dB2 (Inputs: A1, dL/dZ2 in d_dA2_grad)\n",
    "    biasBackwardKernel<<<DIV_UP(DHID, THREADS_PER_BLOCK), THREADS_PER_BLOCK>>>(d_dA2_grad, d_dB2, M, DHID);\n",
    "    dim3 grid_w2(DIV_UP(DHID, 16), DIV_UP(DHID, 16));\n",
    "    gemmBackwardWeightKernel<<<grid_w2, block_mat>>>(d_A1, d_dA2_grad, d_dW2, M, DHID, DHID); // X=A1, K=DHID, N=DHID\n",
    "\n",
    "    // Step 6: Calculate dL/dA1 (gradient to pass to L1)\n",
    "    dim3 grid_a1(DIV_UP(DHID, 16), DIV_UP(M, 16));\n",
    "    gemmBackwardInputKernel<<<grid_a1, block_mat>>>(d_dA2_grad, d_W2, d_dA1_grad, M, DHID, DHID); // K=DHID, N=DHID\n",
    "\n",
    "    // --- L1 BACKWARD (Hidden Layer: DIN -> DHID) ---\n",
    "    // Step 7: ReLU Backward on dL/dA1 to get dL/dZ1 in-place (d_dA1_grad now holds dL/dZ1)\n",
    "    reluBackwardKernel<<<DIV_UP(size_hid, THREADS_PER_BLOCK), THREADS_PER_BLOCK>>>(d_dA1_grad, d_Z1_pre, size_hid); \n",
    "\n",
    "    // Step 8: Calculate dW1 and dB1 (Inputs: X, dL/dZ1 in d_dA1_grad)\n",
    "    biasBackwardKernel<<<DIV_UP(DHID, THREADS_PER_BLOCK), THREADS_PER_BLOCK>>>(d_dA1_grad, d_dB1, M, DHID);\n",
    "    dim3 grid_w1(DIV_UP(DHID, 16), DIV_UP(DIN, 16));\n",
    "    gemmBackwardWeightKernel<<<grid_w1, block_mat>>>(d_X, d_dA1_grad, d_dW1, M, DIN, DHID); // X=d_X, K=DIN, N=DHID\n",
    "\n",
    "    // --- 4. OPTIMIZER STEP (SGD) ---\n",
    "    std::cout << \"\\n--- OPTIMIZER STEP (SGD) ---\" << std::endl;\n",
    "    // L3 Update\n",
    "    updateWeightsKernel<<<DIV_UP(DHID * C, THREADS_PER_BLOCK), THREADS_PER_BLOCK>>>(d_W3, d_dW3, DHID * C, LR);\n",
    "    updateWeightsKernel<<<DIV_UP(C, THREADS_PER_BLOCK), THREADS_PER_BLOCK>>>(d_B3, d_dB3, C, LR);\n",
    "    // L2 Update\n",
    "    updateWeightsKernel<<<DIV_UP(DHID * DHID, THREADS_PER_BLOCK), THREADS_PER_BLOCK>>>(d_W2, d_dW2, DHID * DHID, LR);\n",
    "    updateWeightsKernel<<<DIV_UP(DHID, THREADS_PER_BLOCK), THREADS_PER_BLOCK>>>(d_B2, d_dB2, DHID, LR);\n",
    "    // L1 Update\n",
    "    updateWeightsKernel<<<DIV_UP(DIN * DHID, THREADS_PER_BLOCK), THREADS_PER_BLOCK>>>(d_W1, d_dW1, DIN * DHID, LR);\n",
    "    updateWeightsKernel<<<DIV_UP(DHID, THREADS_PER_BLOCK), THREADS_PER_BLOCK>>>(d_B1, d_dB1, DHID, LR);\n",
    "\n",
    "    // --- 5. Final Output ---\n",
    "    CUDA_CHECK(cudaDeviceSynchronize()); \n",
    "    float final_loss_sum_h;\n",
    "    CUDA_CHECK(cudaMemcpy(&final_loss_sum_h, d_final_loss_sum, sizeof(float), cudaMemcpyDeviceToHost));\n",
    "    float avg_loss = final_loss_sum_h / (float)M;\n",
    "\n",
    "    std::cout << \"\\n--- Training Step Complete ---\" << std::endl;\n",
    "    std::cout << \"Average Cross-Entropy Loss: \" << avg_loss << std::endl;\n",
    "    \n",
    "    // Cleanup (All 3 layers)\n",
    "    cudaFree(d_X); cudaFree(d_Y_true); cudaFree(d_loss_elements); cudaFree(d_final_loss_sum);\n",
    "    cudaFree(d_W1); cudaFree(d_B1); cudaFree(d_Z1_pre); cudaFree(d_A1);\n",
    "    cudaFree(d_W2); cudaFree(d_B2); cudaFree(d_Z2_pre); cudaFree(d_A2);\n",
    "    cudaFree(d_W3); cudaFree(d_B3); cudaFree(d_Z3); \n",
    "    cudaFree(d_dW1); cudaFree(d_dB1); cudaFree(d_dA1_grad); \n",
    "    cudaFree(d_dW2); cudaFree(d_dB2); cudaFree(d_dA2_grad); \n",
    "    cudaFree(d_dW3); cudaFree(d_dB3); cudaFree(d_dZ3_grad);\n",
    "\n",
    "    return 0;\n",
    "}\n",
    "EOF\n",
    "\n",
    "nvcc -std=c++17 full_loop.cu -o full_loop_output && ./full_loop_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a44a88db-31bd-42a4-9c77-950508703c12",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda",
   "language": "python",
   "name": "cuda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
