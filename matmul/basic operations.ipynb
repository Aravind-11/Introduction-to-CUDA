{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae796272-af74-4ab7-9c18-8f548e457ad7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA_HOME = /packages/apps/spack/21/opt/spack/linux-rocky8-zen3/gcc-12.1.0/cuda-12.0.1-x3bnvayrybncl3rqu6zk4zzu4oztblqi\n",
      "which nvcc -> /packages/apps/spack/21/opt/spack/linux-rocky8-zen3/gcc-12.1.0/cuda-12.0.1-x3bnvayrybncl3rqu6zk4zzu4oztblqi/bin/nvcc\n",
      "/packages/apps/spack/21/opt/spack/linux-rocky8-zen3/gcc-12.1.0/cuda-12.0.1-x3bnvayrybncl3rqu6zk4zzu4oztblqi/bin/nvcc\n",
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2023 NVIDIA Corporation\n",
      "Built on Fri_Jan__6_16:45:21_PST_2023\n",
      "Cuda compilation tools, release 12.0, V12.0.140\n",
      "Build cuda_12.0.r12.0/compiler.32267302_0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os, subprocess, shutil\n",
    "\n",
    "# replace this with the nvcc path you posted (we compute CUDA_HOME automatically)\n",
    "nvcc_path = \"/packages/apps/spack/21/opt/spack/linux-rocky8-zen3/gcc-12.1.0/cuda-12.0.1-x3bnvayrybncl3rqu6zk4zzu4oztblqi/bin/nvcc\"\n",
    "\n",
    "if not os.path.exists(nvcc_path):\n",
    "    raise FileNotFoundError(f\"nvcc not found at {nvcc_path} — update nvcc_path if different\")\n",
    "\n",
    "cuda_home = os.path.dirname(os.path.dirname(nvcc_path))\n",
    "os.environ[\"CUDA_HOME\"] = cuda_home\n",
    "os.environ[\"PATH\"] = os.path.join(cuda_home, \"bin\") + os.pathsep + os.environ.get(\"PATH\", \"\")\n",
    "# Append existing LD_LIBRARY_PATH if present\n",
    "os.environ[\"LD_LIBRARY_PATH\"] = os.path.join(cuda_home, \"lib64\") + os.pathsep + os.environ.get(\"LD_LIBRARY_PATH\", \"\")\n",
    "\n",
    "print(\"CUDA_HOME =\", cuda_home)\n",
    "print(\"which nvcc ->\", shutil.which(\"nvcc\"))\n",
    "\n",
    "# quick verify (runs in a shell inheriting these env vars)\n",
    "proc = subprocess.run(\"/bin/bash -lc 'which nvcc && nvcc --version'\", shell=True, capture_output=True, text=True, env=os.environ)\n",
    "print(proc.stdout)\n",
    "if proc.returncode != 0:\n",
    "    print(\"nvcc failed to run; stderr:\\n\", proc.stderr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37be8355-e4a3-4f50-8b1b-fac7a4929ae3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.000000 2.000000 3.000000 4.000000 5.000000 6.000000 7.000000 8.000000 \n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "export CUDA_HOME=/packages/apps/spack/21/opt/spack/linux-rocky8-zen3/gcc-12.1.0/cuda-12.0.1-x3bnvayrybncl3rqu6zk4zzu4oztblqi\n",
    "export PATH=$CUDA_HOME/bin:$PATH\n",
    "export LD_LIBRARY_PATH=$CUDA_HOME/lib64:$LD_LIBRARY_PATH\n",
    "\n",
    "cat > add.cu <<'EOF'\n",
    "#include <cstdio>\n",
    "\n",
    "__global__ void add(int n, float *x, float *y) {\n",
    "  int i = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "  if (i < n) y[i] = x[i] + 1.0f;\n",
    "}\n",
    "\n",
    "int main() {\n",
    "  int N = 16;\n",
    "  float *x, *y;\n",
    "  cudaMallocManaged(&x, N*sizeof(float));\n",
    "  cudaMallocManaged(&y, N*sizeof(float));\n",
    "  for (int i=0;i<N;i++) x[i]=i;\n",
    "  add<<<1,32>>>(N,x,y);\n",
    "  cudaDeviceSynchronize();\n",
    "  for (int i=0;i<8;i++) printf(\"%f \", y[i]);\n",
    "  printf(\"\\n\");\n",
    "  cudaFree(x); cudaFree(y);\n",
    "  return 0;\n",
    "}\n",
    "EOF\n",
    "\n",
    "nvcc -std=c++17 add.cu -o add_test && ./add_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bfbea77f-f2f9-4e53-9867-f70a1605599d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data initialized on Host (CPU).\n",
      "Memory allocated on Device (GPU).\n",
      "Input data copied to Device.\n",
      "Launching kernel: 4096 blocks, 256 threads/block.\n",
      "Result copied back to Host.\n",
      "SUCCESS! Vector addition verified.\n",
      "Example: C[1] = 3 (Expected 3)\n",
      "Example: C[1048575] = 3145725\n",
      "Cleanup complete. Exiting.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cat > vector_add.cu <<'EOF'\n",
    "\n",
    "#include <iostream>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "// Define the size of the arrays\n",
    "const int N = 1 << 20; // 1,048,576 elements\n",
    "\n",
    "// --- KERNEL DEFINITION (Executed on Device) ---\n",
    "// The kernel is the parallel function\n",
    "__global__ void vectorAdd(const int *A, const int *B, int *C, int size) {\n",
    "    // Calculate the unique global index 'i' for this thread\n",
    "    int i = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "\n",
    "    // Safety check: ensure the index is within the array bounds\n",
    "    if (i < size) {\n",
    "        C[i] = A[i] + B[i];\n",
    "    }\n",
    "}\n",
    "\n",
    "// --- HOST MAIN FUNCTION (Executed on CPU) ---\n",
    "int main() {\n",
    "    // --- 0. Setup and Initialization ---\n",
    "    // Memory size in bytes\n",
    "    const int BYTES = N * sizeof(int);\n",
    "\n",
    "    // Host Pointers (CPU memory, allocated with standard malloc)\n",
    "    int *h_A, *h_B, *h_C;\n",
    "    // Device Pointers (GPU memory, allocated with cudaMalloc)\n",
    "    int *d_A, *d_B, *d_C;\n",
    "\n",
    "    // Allocate Host memory\n",
    "    h_A = (int*)malloc(BYTES);\n",
    "    h_B = (int*)malloc(BYTES);\n",
    "    h_C = (int*)malloc(BYTES);\n",
    "\n",
    "    // Initialize Host arrays A and B\n",
    "    for (int i = 0; i < N; i++) {\n",
    "        h_A[i] = i;      // A = [0, 1, 2, 3, ...]\n",
    "        h_B[i] = i * 2;  // B = [0, 2, 4, 6, ...]\n",
    "    }\n",
    "    std::cout << \"Data initialized on Host (CPU).\" << std::endl;\n",
    "\n",
    "    // 1. MEMORY ALLOCATION (Device/GPU)\n",
    "    cudaMalloc((void**)&d_A, BYTES);\n",
    "    cudaMalloc((void**)&d_B, BYTES);\n",
    "    cudaMalloc((void**)&d_C, BYTES);\n",
    "    std::cout << \"Memory allocated on Device (GPU).\" << std::endl;\n",
    "\n",
    "    // 2. DATA TRANSFER (Host -> Device)\n",
    "    // Copy the initialized input arrays A and B from CPU to GPU\n",
    "    cudaMemcpy(d_A, h_A, BYTES, cudaMemcpyHostToDevice);\n",
    "    cudaMemcpy(d_B, h_B, BYTES, cudaMemcpyHostToDevice);\n",
    "    std::cout << \"Input data copied to Device.\" << std::endl;\n",
    "    \n",
    "    // --- 3. KERNEL LAUNCH SETUP ---\n",
    "    // Define the execution configuration\n",
    "    int threadsPerBlock = 256; \n",
    "    // Calculate the grid size (number of blocks) using ceiling division\n",
    "    int blocksPerGrid = (N + threadsPerBlock - 1) / threadsPerBlock; \n",
    "\n",
    "    // 4. KERNEL LAUNCH (The work begins on the GPU)\n",
    "    std::cout << \"Launching kernel: \" << blocksPerGrid << \" blocks, \" \n",
    "              << threadsPerBlock << \" threads/block.\" << std::endl;\n",
    "\n",
    "    vectorAdd<<<blocksPerGrid, threadsPerBlock>>>(d_A, d_B, d_C, N);\n",
    "\n",
    "    // Wait for the GPU to finish execution\n",
    "    cudaDeviceSynchronize();\n",
    "    \n",
    "    // 5. DATA TRANSFER (Device -> Host)\n",
    "    // Copy the result array C back from GPU to CPU\n",
    "    cudaMemcpy(h_C, d_C, BYTES, cudaMemcpyDeviceToHost);\n",
    "    std::cout << \"Result copied back to Host.\" << std::endl;\n",
    "    \n",
    "    // --- 6. Validation and Output (Host) ---\n",
    "    // Check first few results: C[i] = i + 2*i = 3*i\n",
    "    if (h_C[0] == 0 && h_C[1] == 3 && h_C[N-1] == 3 * (N-1)) {\n",
    "        std::cout << \"SUCCESS! Vector addition verified.\" << std::endl;\n",
    "        std::cout << \"Example: C[1] = \" << h_C[1] << \" (Expected 3)\" << std::endl;\n",
    "        std::cout << \"Example: C[\" << N-1 << \"] = \" << h_C[N-1] << std::endl;\n",
    "    } else {\n",
    "        std::cerr << \"FAILURE! Results did not match.\" << std::endl;\n",
    "    }\n",
    "\n",
    "    // 7. CLEANUP\n",
    "    // Free Host memory\n",
    "    free(h_A); \n",
    "    free(h_B); \n",
    "    free(h_C); \n",
    "    // Free Device memory\n",
    "    cudaFree(d_A); \n",
    "    cudaFree(d_B); \n",
    "    cudaFree(d_C); \n",
    "    std::cout << \"Cleanup complete. Exiting.\" << std::endl;\n",
    "\n",
    "    return 0;\n",
    "}\n",
    "\n",
    "EOF\n",
    "\n",
    "nvcc -std=c++17 vector_add.cu -o vector_add_test && ./vector_add_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c74903e2-afd5-4b65-83aa-92327d527072",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU time: 2.81444 ms\n",
      "GPU time (HtoD + kernel + DtoH): 3.7577 ms\n",
      "Speedup (CPU/GPU): 0.74898x\n",
      "Result OK\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cat > vector_add_compare.cu <<'EOF'\n",
    "#include <iostream>\n",
    "#include <cuda_runtime.h>\n",
    "#include <chrono>\n",
    "\n",
    "const int N = 1 << 20;  // 1,048,576 elements\n",
    "\n",
    "__global__ void vectorAdd(const int *A, const int *B, int *C, int size) {\n",
    "    int i = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (i < size) C[i] = A[i] + B[i];\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    const int BYTES = N * sizeof(int);\n",
    "\n",
    "    int *h_A = (int*)malloc(BYTES);\n",
    "    int *h_B = (int*)malloc(BYTES);\n",
    "    int *h_C = (int*)malloc(BYTES);\n",
    "\n",
    "    for (int i = 0; i < N; i++) {\n",
    "        h_A[i] = i;\n",
    "        h_B[i] = i * 2;\n",
    "    }\n",
    "\n",
    "    // ---------------- CPU TIMING ----------------\n",
    "    auto cpu_start = std::chrono::high_resolution_clock::now();\n",
    "    for (int i = 0; i < N; i++) {\n",
    "        h_C[i] = h_A[i] + h_B[i];\n",
    "    }\n",
    "    auto cpu_end = std::chrono::high_resolution_clock::now();\n",
    "    double cpu_ms = std::chrono::duration<double, std::milli>(cpu_end - cpu_start).count();\n",
    "    std::cout << \"CPU time: \" << cpu_ms << \" ms\" << std::endl;\n",
    "\n",
    "    // ---------------- GPU -----------------------\n",
    "    int *d_A, *d_B, *d_C;\n",
    "    cudaMalloc((void**)&d_A, BYTES);\n",
    "    cudaMalloc((void**)&d_B, BYTES);\n",
    "    cudaMalloc((void**)&d_C, BYTES);\n",
    "\n",
    "    auto gpu_start = std::chrono::high_resolution_clock::now();\n",
    "\n",
    "    cudaMemcpy(d_A, h_A, BYTES, cudaMemcpyHostToDevice);\n",
    "    cudaMemcpy(d_B, h_B, BYTES, cudaMemcpyHostToDevice);\n",
    "\n",
    "    int threads = 256;\n",
    "    int blocks = (N + threads - 1) / threads;\n",
    "\n",
    "    vectorAdd<<<blocks, threads>>>(d_A, d_B, d_C, N);\n",
    "    cudaDeviceSynchronize();\n",
    "\n",
    "    cudaMemcpy(h_C, d_C, BYTES, cudaMemcpyDeviceToHost);\n",
    "\n",
    "    auto gpu_end = std::chrono::high_resolution_clock::now();\n",
    "    double gpu_ms = std::chrono::duration<double, std::milli>(gpu_end - gpu_start).count();\n",
    "\n",
    "    std::cout << \"GPU time (HtoD + kernel + DtoH): \" << gpu_ms << \" ms\" << std::endl;\n",
    "    std::cout << \"Speedup (CPU/GPU): \" << cpu_ms / gpu_ms << \"x\" << std::endl;\n",
    "\n",
    "    // Validate last result\n",
    "    if (h_C[N-1] == 3*(N-1)) std::cout << \"Result OK\\n\";\n",
    "    else std::cout << \"Wrong result!\\n\";\n",
    "\n",
    "    free(h_A); free(h_B); free(h_C);\n",
    "    cudaFree(d_A); cudaFree(d_B); cudaFree(d_C);\n",
    "}\n",
    "EOF\n",
    "\n",
    "nvcc -std=c++17 vector_add_compare.cu -o vector_add_compare\n",
    "./vector_add_compare\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "46f7dc33-c86d-412d-8808-560439cd9679",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data initialized on Host. Expected Sum: 1048576\n",
      "Input data copied to Device. K (Partial Sums) = 4096\n",
      "Pass 1 complete. Partial sums created.\n",
      "Pass 2 complete. Final sum calculated.\n",
      "Cleanup complete. Exiting.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "❌ FAILURE! Expected 1048576, Got 65536\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cat > reduce_sum.cu <<'EOF'\n",
    "#include <iostream>\n",
    "#include <cuda_runtime.h>\n",
    "#include <cmath>\n",
    "\n",
    "// Define the size and thread configuration\n",
    "const int N = 1 << 20;     // 1,048,576 elements\n",
    "const int THREADS_PER_BLOCK = 256; \n",
    "\n",
    "// --- KERNEL DEFINITION (Assumed to be defined here, copied from prior context) ---\n",
    "__global__ void reduceSum(int *g_input, int *g_output, int size) {\n",
    "    // Shared Memory declaration (size of the block)\n",
    "    extern __shared__ int s_data[]; \n",
    "    \n",
    "    int tid = threadIdx.x; \n",
    "    int i = blockIdx.x * blockDim.x + tid; // Global index\n",
    "\n",
    "    // 1. Data Staging (Global -> Shared)\n",
    "    if (i < size) {\n",
    "        s_data[tid] = g_input[i];\n",
    "    } else {\n",
    "        s_data[tid] = 0; // Handle out-of-bounds safety\n",
    "    }\n",
    "\n",
    "    __syncthreads(); \n",
    "\n",
    "    // 2. Reduction Loop (Parallel Summation)\n",
    "    for (int stride = blockDim.x / 2; stride > 0; stride /= 2) {\n",
    "        if (tid < stride) {\n",
    "            s_data[tid] += s_data[tid + stride];\n",
    "        }\n",
    "        __syncthreads(); \n",
    "    }\n",
    "    \n",
    "    // 3. Write Result (Only Thread 0 writes the final block sum)\n",
    "    if (tid == 0) {\n",
    "        g_output[blockIdx.x] = s_data[0]; \n",
    "    }\n",
    "}\n",
    "// --- END KERNEL DEFINITION ---\n",
    "\n",
    "\n",
    "// --- HOST MAIN FUNCTION (Executed on CPU) ---\n",
    "int main() {\n",
    "    // --- 0. Setup and Initialization ---\n",
    "    const int BYTES = N * sizeof(int);\n",
    "    \n",
    "    // Host Pointers\n",
    "    int *h_A, *h_PartialSum, *h_FinalSum; \n",
    "    \n",
    "    // Device Pointers\n",
    "    int *d_A, *d_PartialSums, *d_FinalSum;\n",
    "\n",
    "    // Allocate Host memory\n",
    "    h_A = (int*)malloc(BYTES);\n",
    "    h_FinalSum = (int*)malloc(sizeof(int));\n",
    "\n",
    "    // Initialize Host array A (e.g., A = [1, 1, 1, ..., 1]. Expected sum = N)\n",
    "    long long expected_sum = 0;\n",
    "    for (int i = 0; i < N; i++) {\n",
    "        h_A[i] = 1; \n",
    "        expected_sum += h_A[i];\n",
    "    }\n",
    "    std::cout << \"Data initialized on Host. Expected Sum: \" << expected_sum << std::endl;\n",
    "\n",
    "    // 1. MEMORY ALLOCATION (Device/GPU)\n",
    "    cudaMalloc((void**)&d_A, BYTES);\n",
    "\n",
    "    // Calculate number of blocks for Pass 1 (K)\n",
    "    int K = (N + THREADS_PER_BLOCK - 1) / THREADS_PER_BLOCK; \n",
    "    \n",
    "    // Allocate Device memory for K partial sums and 1 final sum\n",
    "    cudaMalloc((void**)&d_PartialSums, K * sizeof(int));\n",
    "    cudaMalloc((void**)&d_FinalSum, 1 * sizeof(int));\n",
    "    \n",
    "    // Allocate Host memory for partial sums result check (optional)\n",
    "    h_PartialSum = (int*)malloc(K * sizeof(int));\n",
    "\n",
    "    // 2. DATA TRANSFER (Host -> Device)\n",
    "    cudaMemcpy(d_A, h_A, BYTES, cudaMemcpyHostToDevice);\n",
    "    std::cout << \"Input data copied to Device. K (Partial Sums) = \" << K << std::endl;\n",
    "\n",
    "    // --- PASS 1: Reduce N elements to K partial sums ---\n",
    "    // Execution Configuration: K blocks, 256 threads/block. Shared memory size: 256 * sizeof(int)\n",
    "    reduceSum<<<K, THREADS_PER_BLOCK, THREADS_PER_BLOCK * sizeof(int)>>>(d_A, d_PartialSums, N); \n",
    "    cudaDeviceSynchronize();\n",
    "    \n",
    "    std::cout << \"Pass 1 complete. Partial sums created.\" << std::endl;\n",
    "\n",
    "    // --- PASS 2: Reduce K partial sums to 1 final sum ---\n",
    "    \n",
    "    // Calculate blocks needed for Pass 2\n",
    "    int blocksForPass2 = (K + THREADS_PER_BLOCK - 1) / THREADS_PER_BLOCK; \n",
    "\n",
    "    // If K is small (<= 1 block size), we can write the final result directly.\n",
    "    // Otherwise, we use d_PartialSums as the input and d_FinalSum as the output.\n",
    "    reduceSum<<<blocksForPass2, THREADS_PER_BLOCK, THREADS_PER_BLOCK * sizeof(int)>>>(d_PartialSums, d_FinalSum, K);\n",
    "    cudaDeviceSynchronize();\n",
    "\n",
    "    std::cout << \"Pass 2 complete. Final sum calculated.\" << std::endl;\n",
    "\n",
    "    // 3. DATA TRANSFER (Device -> Host)\n",
    "    // Copy the single final result back to the host\n",
    "    cudaMemcpy(h_FinalSum, d_FinalSum, sizeof(int), cudaMemcpyDeviceToHost); \n",
    "    \n",
    "    // --- 4. Validation and Output (Host) ---\n",
    "    if (*h_FinalSum == expected_sum) {\n",
    "        std::cout << \"✅ SUCCESS! Total Sum Verified: \" << *h_FinalSum << std::endl;\n",
    "    } else {\n",
    "        std::cerr << \"❌ FAILURE! Expected \" << expected_sum << \", Got \" << *h_FinalSum << std::endl;\n",
    "    }\n",
    "\n",
    "    // 5. CLEANUP\n",
    "    free(h_A); \n",
    "    free(h_PartialSum); \n",
    "    free(h_FinalSum); \n",
    "    cudaFree(d_A); \n",
    "    cudaFree(d_PartialSums); \n",
    "    cudaFree(d_FinalSum); \n",
    "    std::cout << \"Cleanup complete. Exiting.\" << std::endl;\n",
    "\n",
    "    return 0;\n",
    "}\n",
    "\n",
    "EOF\n",
    "\n",
    "nvcc -std=c++17 reduce_sum.cu -o reduce_sum_output && ./reduce_sum_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddcf7f6b-e699-433b-8694-297e492e8a5d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda",
   "language": "python",
   "name": "cuda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
